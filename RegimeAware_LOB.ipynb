{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56df3dc2",
   "metadata": {},
   "source": [
    "# Regime-Aware LOB Midterm\n",
    "\n",
    "This notebook does the following...\n",
    "\n",
    "- Handles either 1-second or 1-minute LOB CSVs\n",
    "\n",
    "- Builds robust features (depth, imbalances, flows, realized vol)\n",
    "\n",
    "- Creates labels for short-horizon mid-price direction (±1 / 0)\n",
    "\n",
    "- Discovers regimes with **HMM** (if available) or **KMeans** fallback\n",
    "\n",
    "- Trains **Logistic Regression** and **XGBoost** (or RandomForest fallback)\n",
    "\n",
    "- Evaluates via **walk-forward** splits with hour-based windows to get *many folds*\n",
    "\n",
    "- Prints compact metrics + one clean bar chart (balanced vs toxic hit-rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969c604c",
   "metadata": {},
   "outputs": [],
   "source": "import warnings, os, sys, math\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nwarnings.filterwarnings(\"ignore\")\n\nfrom hmmlearn.hmm import GaussianHMM\nfrom xgboost import XGBClassifier\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.utils.class_weight import compute_sample_weight\nfrom sklearn.metrics import accuracy_score, f1_score"
  },
  {
   "cell_type": "markdown",
   "id": "c7a34f6b",
   "metadata": {},
   "source": [
    "## 0. Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5495c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_CSV = 'BTC_1sec.csv'\n",
    "\n",
    "TIME_COL = 'timestamp'\n",
    "MID_COL = 'midpoint'\n",
    "SPREAD_COL = 'spread'\n",
    "\n",
    "TOP_K = 15\n",
    "\n",
    "HORIZON_SEC = 30\n",
    "\n",
    "THRESH_BPS = 1.0\n",
    "\n",
    "TRAIN_DAYS  = 3\n",
    "TEST_HOURS  = 4\n",
    "EMBARGO_SEC = 60\n",
    "\n",
    "STRIDE_HRS  = 12\n",
    "\n",
    "USE_PCA = True\n",
    "PCA_VARIANCE = 0.95\n",
    "\n",
    "MAX_TRAIN_SAMPLES = 120000\n",
    "HMM_COVARIANCE = 'diag'\n",
    "HMM_ITERATIONS = 150\n",
    "XGB_N_ESTIMATORS = 300\n",
    "XGB_MAX_DEPTH = 6\n",
    "XGB_LEARNING_RATE = 0.05\n",
    "XGB_EARLY_STOPPING = 30\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d5b450",
   "metadata": {},
   "source": [
    "## 1. Load & Inspect\n",
    "\n",
    "The data we are using is the following Kaggle dataset (https://www.kaggle.com/datasets/martinsn/high-frequency-crypto-limit-order-book-data). We download and add the data to the project locally as the dataset is to big to push to Github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e37ed59",
   "metadata": {},
   "outputs": [],
   "source": "df = pd.read_csv(DATA_CSV)\n\nif TIME_COL not in df.columns:\n    for c in df.columns:\n        if 'time' in c.lower():\n            TIME_COL = c\n            break\n\ndf[TIME_COL] = pd.to_datetime(df[TIME_COL], utc=True, errors='coerce')\ndf = df.dropna(subset=[TIME_COL]).sort_values(TIME_COL).reset_index(drop=True)\n\nprint(\"Rows:\", len(df))\nprint(\"Time range:\", df[TIME_COL].min(), \"→\", df[TIME_COL].max())\nprint(\"Columns:\", list(df.columns)[:12], \"...\")"
  },
  {
   "cell_type": "markdown",
   "id": "8501c400",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering\n",
    "\n",
    "The raw data contains features such as midpoint, buys, sells, spread, and much more. Some features we require such as the AskDepth for example can be calculated easily which is what we'll do here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5668c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def level_cols(prefix, k=TOP_K, in_df=None):\n",
    "    cols = [f\"{prefix}_{i}\" for i in range(k)]\n",
    "    if in_df is not None:\n",
    "        cols = [c for c in cols if c in in_df.columns]\n",
    "    return cols\n",
    "\n",
    "def agg_levels(df, prefix, k=TOP_K):\n",
    "    cols = level_cols(prefix, k, in_df=df)\n",
    "    if not cols:\n",
    "        return pd.Series(0.0, index=df.index)\n",
    "    return df[cols].sum(axis=1)\n",
    "\n",
    "df['spread_pct'] = (df[SPREAD_COL] / df[MID_COL]).replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "df['BidDepth_k'] = agg_levels(df, \"bids_limit_notional\", TOP_K)\n",
    "df['AskDepth_k'] = agg_levels(df, \"asks_limit_notional\", TOP_K)\n",
    "df['DepthImb_k'] = (df['BidDepth_k'] - df['AskDepth_k']) / (df['BidDepth_k'] + df['AskDepth_k'] + 1e-9)\n",
    "df['MO_bid_tau'] = agg_levels(df, \"bids_market_notional\", TOP_K)\n",
    "df['MO_ask_tau'] = agg_levels(df, \"asks_market_notional\", TOP_K)\n",
    "df['MO_imb_tau'] = (df['MO_ask_tau'] - df['MO_bid_tau']) / (df['MO_ask_tau'] + df['MO_bid_tau'] + 1e-9)\n",
    "df['CA_bid_tau'] = agg_levels(df, \"bids_cancel_notional\", TOP_K)\n",
    "df['CA_ask_tau'] = agg_levels(df, \"asks_cancel_notional\", TOP_K)\n",
    "df['CA_imb_tau'] = (df['CA_ask_tau'] - df['CA_bid_tau']) / (df['CA_ask_tau'] + df['CA_bid_tau'] + 1e-9)\n",
    "\n",
    "ret = df[MID_COL].pct_change()\n",
    "df['rv_5s']  = ret.rolling(5, min_periods=1).std().fillna(0)\n",
    "\n",
    "feature_cols = [\n",
    "    'spread_pct','DepthImb_k','MO_imb_tau','CA_imb_tau','rv_5s',\n",
    "    'BidDepth_k','AskDepth_k','MO_bid_tau','MO_ask_tau','CA_bid_tau','CA_ask_tau'\n",
    "]\n",
    "print(pd.DataFrame({'exists':[c in df.columns for c in feature_cols]}, index=feature_cols))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131d1e00",
   "metadata": {},
   "source": [
    "## 3. Labels (short-horizon direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfefc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df.sort_values(TIME_COL).reset_index(drop=True)\n",
    "approx_dt = (df[TIME_COL].iloc[1] - df[TIME_COL].iloc[0]).total_seconds()\n",
    "shift_n = max(1, int(round(HORIZON_SEC / max(1, approx_dt))))\n",
    "df['mid_fwd'] = df[MID_COL].shift(-shift_n)\n",
    "\n",
    "thresh = THRESH_BPS * 1e-4\n",
    "r = (df['mid_fwd'] - df[MID_COL]) / df[MID_COL]\n",
    "y = np.where(r >  thresh,  1, np.where(r < -thresh, -1, 0))\n",
    "df['y'] = y\n",
    "\n",
    "df_lbl = df.dropna(subset=['mid_fwd']).copy()\n",
    "print(\"Label distribution:\", pd.Series(df_lbl['y']).value_counts(normalize=True).round(3).to_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983e642a",
   "metadata": {},
   "source": [
    "## 4. Scaling helper\n",
    "This function standardizes features in the training/testing datasets so that each feature has a mean of 0 and a standard deviation of 1 by fitting scaling parameters on the training data and applies the same transformation to the test data, ensuring consistency and preventing data leakage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81773e5",
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.preprocessing import StandardScaler\n\ndef scale_features(df_train, df_test, cols=None, use_pca=USE_PCA, pca_var=PCA_VARIANCE):\n    if cols is None:\n        cols = feature_cols\n    scaler = StandardScaler()\n    Xtr = scaler.fit_transform(df_train[cols].fillna(0.0).values)\n    Xte = scaler.transform(df_test[cols].fillna(0.0).values)\n    \n    pca = None\n    if use_pca:\n        pca = PCA(n_components=pca_var, random_state=RANDOM_STATE)\n        Xtr = pca.fit_transform(Xtr)\n        Xte = pca.transform(Xte)\n    \n    return Xtr, Xte, pca"
  },
  {
   "cell_type": "markdown",
   "id": "c9d6c65b",
   "metadata": {},
   "source": [
    "## 5. Walk-forward splitters\n",
    "This helper function generates rolling train-test time splits for time-series cross-validation using a walk-forward approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24288f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def walk_forward_splits_hours_v2(\n",
    "    timestamps, train_days=3, test_hours=4, embargo_sec=60, stride_hours=None\n",
    "):\n",
    "    ts = pd.to_datetime(timestamps, utc=True).sort_values()\n",
    "    tmin = ts.min()\n",
    "    tmax = ts.max()\n",
    "    stride = pd.Timedelta(hours=(stride_hours or test_hours))\n",
    "\n",
    "    cur = tmin\n",
    "    while True:\n",
    "        tr_s = cur\n",
    "        tr_e = tr_s + pd.Timedelta(days=train_days)\n",
    "        te_s = tr_e + pd.Timedelta(seconds=embargo_sec)\n",
    "        te_e = te_s + pd.Timedelta(hours=test_hours)\n",
    "        if te_e > tmax:\n",
    "            break\n",
    "        yield (tr_s, tr_e, te_s, te_e)\n",
    "        cur = cur + stride\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26851818",
   "metadata": {},
   "source": [
    "## 6. Regime discovery (HMM or KMeans fallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9b83af",
   "metadata": {},
   "outputs": [],
   "source": "def fit_hmm(X_train, df_train, n_states=2, random_state=42):\n    tox_score_cols = ['MO_imb_tau','CA_imb_tau','DepthImb_k']\n    train_score = df_train[tox_score_cols].fillna(0).copy()\n    train_score['tox_score'] = (\n        train_score['MO_imb_tau'].abs()\n      + train_score['CA_imb_tau'].abs()\n      + train_score['DepthImb_k'].abs()\n    )\n\n    hmm = GaussianHMM(\n        n_components=n_states, \n        covariance_type=HMM_COVARIANCE,\n        n_iter=HMM_ITERATIONS,\n        random_state=random_state\n    )\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", message=\"Model is not converging\")\n        hmm.fit(X_train)\n    states = hmm.predict(X_train)\n    state_avg = pd.DataFrame({'state': states, 'tox': train_score['tox_score'].values}).groupby('state')['tox'].mean()\n    tox_state = int(state_avg.idxmax())\n    return hmm, tox_state"
  },
  {
   "cell_type": "markdown",
   "id": "67126975",
   "metadata": {},
   "source": [
    "## 7. Supervised models (baseline vs regime-aware)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43da9079",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CLASSES = [-1,0,1]\n",
    "ENC_MAP = {-1:0, 0:1, 1:2}\n",
    "DEC_MAP = {v:k for k,v in ENC_MAP.items()}\n",
    "\n",
    "def metrics_from_proba_encoded(proba, y_true_enc):\n",
    "    yhat_enc = proba.argmax(axis=1)\n",
    "    yhat = np.array([DEC_MAP[i] for i in yhat_enc])\n",
    "    ytrue = np.array([DEC_MAP[i] for i in y_true_enc])\n",
    "    acc = accuracy_score(ytrue, yhat)\n",
    "    f1  = f1_score(ytrue, yhat, average='macro')\n",
    "    return acc, f1, yhat, ytrue\n",
    "\n",
    "def regime_hits(yhat, ytrue, regime_flag):\n",
    "    bal = (regime_flag==0)\n",
    "    tox = (regime_flag==1)\n",
    "    hr_bal = float((yhat[bal]==ytrue[bal]).mean()) if bal.any() else np.nan\n",
    "    hr_tox = float((yhat[tox]==ytrue[tox]).mean()) if tox.any() else np.nan\n",
    "    return hr_bal, hr_tox\n",
    "\n",
    "df_sup = df_lbl.dropna(subset=['y']).copy().sort_values(TIME_COL)\n",
    "folds = list(walk_forward_splits_hours_v2(df_sup[TIME_COL], TRAIN_DAYS, TEST_HOURS, EMBARGO_SEC, STRIDE_HRS))\n",
    "print(\"Planned folds:\", len(folds))\n",
    "for i,(a,b,c,d) in enumerate(folds[:5]):\n",
    "    print(f\"Fold {i}: TRAIN {a}→{b}  TEST {c}→{d}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b49e5de",
   "metadata": {},
   "source": [
    "## 8. Walk-forward evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80a9915",
   "metadata": {},
   "outputs": [],
   "source": "import time\n\nresults = []\nfold = 0\nMIN_TRAIN = 300\nMIN_TEST  = 100\n\nfor (tr_s, tr_e, te_s, te_e) in folds:\n    fold_start = time.time()\n    train = df_sup[(df_sup[TIME_COL] >= tr_s) & (df_sup[TIME_COL] < tr_e)].copy()\n    test  = df_sup[(df_sup[TIME_COL] >= te_s) & (df_sup[TIME_COL] < te_e)].copy()\n    if len(train) < MIN_TRAIN or len(test) < MIN_TEST:\n        continue\n\n    if len(train) > MAX_TRAIN_SAMPLES:\n        train = train.sample(n=MAX_TRAIN_SAMPLES, random_state=RANDOM_STATE)\n        print(f\"\\n[Fold {fold+1}/{len(folds)}] Train: {len(train):,} samples (subsampled), Test: {len(test):,}\")\n    else:\n        print(f\"\\n[Fold {fold+1}/{len(folds)}] Train: {len(train):,} samples, Test: {len(test):,}\")\n    \n    t0 = time.time()\n    Xtr_base, Xte_base, pca_model = scale_features(train, test)\n    if USE_PCA and pca_model is not None and fold == 0:\n        print(f\"PCA: {len(feature_cols)} → {pca_model.n_components_} components ({PCA_VARIANCE*100}% var)\")\n    print(f\"  Scaling+PCA: {time.time()-t0:.1f}s\")\n    \n    ytr_enc = np.array([ENC_MAP[v] for v in train['y'].values])\n    yte_enc = np.array([ENC_MAP[v] for v in test['y'].values])\n\n    t0 = time.time()\n    lr_base = LogisticRegression(max_iter=1000, C=0.5, class_weight='balanced', solver='lbfgs')\n    lr_base.fit(Xtr_base, ytr_enc)\n    acc_lr_base, f1_lr_base, _, _ = metrics_from_proba_encoded(lr_base.predict_proba(Xte_base), yte_enc)\n    print(f\"  LR baseline: {time.time()-t0:.1f}s\")\n\n    t0 = time.time()\n    w_base = compute_sample_weight(class_weight='balanced', y=ytr_enc)\n    xgb_base = XGBClassifier(\n        n_estimators=XGB_N_ESTIMATORS,\n        max_depth=XGB_MAX_DEPTH,\n        learning_rate=XGB_LEARNING_RATE,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        objective='multi:softprob',\n        num_class=3,\n        tree_method='hist',\n        reg_lambda=1.0,\n        random_state=42,\n        early_stopping_rounds=XGB_EARLY_STOPPING\n    )\n    split_idx = int(0.8 * len(Xtr_base))\n    xgb_base.fit(\n        Xtr_base[:split_idx], ytr_enc[:split_idx],\n        sample_weight=w_base[:split_idx],\n        eval_set=[(Xtr_base[split_idx:], ytr_enc[split_idx:])],\n        verbose=False\n    )\n    acc_xgb_base, f1_xgb_base, _, _ = metrics_from_proba_encoded(xgb_base.predict_proba(Xte_base), yte_enc)\n    print(f\"  XGB baseline: {time.time()-t0:.1f}s\")\n\n    t0 = time.time()\n    model_reg, tox_state = fit_hmm(Xtr_base, train, n_states=2)\n    print(f\"  HMM: {time.time()-t0:.1f}s\")\n    \n    s_tr = model_reg.predict(Xtr_base)\n    s_te = model_reg.predict(Xte_base)\n\n    train['regime_hmm'] = (s_tr == tox_state).astype(int)\n    test['regime_hmm']  = (s_te == tox_state).astype(int)\n\n    Xtr = np.c_[Xtr_base, train['regime_hmm'].values]\n    Xte = np.c_[Xte_base, test['regime_hmm'].values]\n\n    t0 = time.time()\n    lr = LogisticRegression(max_iter=1000, C=0.5, class_weight='balanced', solver='lbfgs')\n    lr.fit(Xtr, ytr_enc)\n    acc_lr, f1_lr, yhat_lr, ytrue_lr = metrics_from_proba_encoded(lr.predict_proba(Xte), yte_enc)\n    print(f\"  LR regime-aware: {time.time()-t0:.1f}s\")\n\n    t0 = time.time()\n    w = compute_sample_weight(class_weight='balanced', y=ytr_enc)\n    xgb = XGBClassifier(\n        n_estimators=XGB_N_ESTIMATORS,\n        max_depth=XGB_MAX_DEPTH,\n        learning_rate=XGB_LEARNING_RATE,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        objective='multi:softprob',\n        num_class=3,\n        tree_method='hist',\n        reg_lambda=1.0,\n        random_state=42,\n        early_stopping_rounds=XGB_EARLY_STOPPING\n    )\n    split_idx = int(0.8 * len(Xtr))\n    xgb.fit(\n        Xtr[:split_idx], ytr_enc[:split_idx],\n        sample_weight=w[:split_idx],\n        eval_set=[(Xtr[split_idx:], ytr_enc[split_idx:])],\n        verbose=False\n    )\n    acc_xgb, f1_xgb, yhat_main, ytrue_main = metrics_from_proba_encoded(xgb.predict_proba(Xte), yte_enc)\n    print(f\"  XGB regime-aware: {time.time()-t0:.1f}s\")\n\n    hr_bal, hr_tox = regime_hits(yhat_main, ytrue_main, test['regime_hmm'].values)\n\n    results.append({\n        'fold': fold,\n        'test_start': te_s, 'test_end': te_e,\n        'acc_lr_base': acc_lr_base,   'f1_lr_base': f1_lr_base,\n        'acc_xgb_base': acc_xgb_base, 'f1_xgb_base': f1_xgb_base,\n        'acc_lr': acc_lr,   'f1_lr': f1_lr,\n        'acc_xgb': acc_xgb, 'f1_xgb': f1_xgb,\n        'hr_bal': hr_bal, 'hr_tox': hr_tox,\n        'lift': hr_tox - hr_bal,\n        'pct_toxic_test': float((test['regime_hmm'] == 1).mean())\n    })\n    \n    print(f\"  FOLD TOTAL: {time.time()-fold_start:.1f}s\")\n    fold += 1\n\ndf_res = pd.DataFrame(results)\nprint(\"\\nActual folds evaluated:\", len(df_res))\nif len(df_res)==0:\n    raise RuntimeError(\"No folds evaluated.\")\n\ncols = ['fold','test_start','test_end','acc_lr_base','acc_lr','acc_xgb_base','acc_xgb','hr_bal','hr_tox','lift','pct_toxic_test']\ndisplay(df_res[cols].round(3))\n\nprint(\"\\nMeans across folds:\")\ndisplay(df_res[['acc_lr_base','acc_lr','acc_xgb_base','acc_xgb','hr_bal','hr_tox','lift']].mean().round(3))"
  },
  {
   "cell_type": "markdown",
   "id": "f03ceb4f",
   "metadata": {},
   "source": [
    "## 9. Plot: Regime-conditioned hit-rate per fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e7195d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=(7.5,4.2))\n",
    "x = np.arange(len(df_res))\n",
    "ax.bar(x - 0.2, df_res['hr_bal'], width=0.4, label='Balanced', alpha=0.85)\n",
    "ax.bar(x + 0.2, df_res['hr_tox'], width=0.4, label='Toxic',    alpha=0.85)\n",
    "\n",
    "labels = pd.to_datetime(df_res['test_start']).dt.strftime('%m-%d %H:%M')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels, rotation=40, ha='right')\n",
    "ax.set_xlabel(\"Fold (Test Window Start)\")\n",
    "ax.set_ylabel(\"Hit Rate\")\n",
    "ax.set_title(f\"Regime-Conditioned Hit Rate • Mean Lift = {df_res['lift'].mean():.3f}\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}